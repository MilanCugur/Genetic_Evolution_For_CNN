% Simposium MATHEMATICS and APPLICATIONS
% Requires Latex2e!
\documentclass[eng]{simposium}
\volume{,Vol. X}  %%%%%% TO BE ENTERED BY THE EDITOR(S)
\issue{(1)}      %%%%%% TO BE ENTERED BY THE EDITOR(S)
\pubyear{2019}   %%%%%% TO BE ENTERED BY THE EDITOR(S)
\firstpage{1}    %%%%%% TO BE ENTERED BY THE EDITOR(S)
\lastpage{12}     %%%%%% TO BE ENTERED BY THE EDITOR(S)

%%%%%% ENTER HERE ADDITIONAL PACKAGES
%%%%%% For example: \usepackage{gclc}
\usepackage{xcolor}
\usepackage[ruled,noline]{algorithm2e}

%%%%%% ENTER HERE YOUR OWN LATEX COMMANDS
%%%%%% For example: \newcommand{\const}{\mathop{\mathrm{const}}}

\begin{document}
\begin{frontmatter}

\title{Modified hybrid genetic algorithm for training convolutional neural networks}

\author{\textbf{\fnms{Milan M.} \snm{Čugurović}}}
\address{Faculty of Mathematics, University of Belgrade, Studentski trg 16, 11000 Belgrade\\
\email{milan\_cugurovic@matf.bg.ac.rs}
}
\author{\textbf{\fnms{Nikola} \snm{Dimitrijević}}}
\address{Microsoft Development Center Serbia, Španskih boraca 3, 11000 Belgrade\\
\email{nikoladim95@gmail.com}
}
\author{\textbf{\fnms{Stefan} \snm{Mišković}}}
\address{Faculty of Mathematics, University of Belgrade, Studentski trg 16, 11000 Belgrade\\
\email{stefan@matf.bg.ac.rs}
}

\received{\smonth{December} \syear{2019}}   %%%%%% TO BE ENTERED BY THE EDITOR(S)

\maketitle
\begin{abstract}
    
This paper presents a modified variant of genetic algorithm for training convolutional architectures which reduces the execution time of the algorithm. 
Modification is based on changing the evolutional segment of the algorithm by focusing on limiting the training time of each individual and incorporating the 
learnt knowledge of neuron parameters from the previous generations into each new one. By doing so the evolution is made more efficient, thus reducing the time 
needed to find the desired architecture.

Additional contribution of this paper is creating new dataset \textit{DoubledMNIST}, which represents a successor of the popular MNIST dataset.
Created dataset is doubled with respect to the MNIST dataset both in terms of the number of instances and in terms of the resolution of each individual isntance.
Results shown in the paper were obtained using the presented improved method on the created dataset. The paper also shows classification results on the said dataset.
\end{abstract}

\begin{keyword}
genetic algorithm; CNN arhitectures; MNIST dataset; DoubledMNIST dataset
\end{keyword}
\end{frontmatter}

\section{Introduction}

\section{Related work}

This section provides background about offline handwritten character datasets and about incorporating genetic algorithms 
with the learning of CNN architectures, and their training. 

\subsection{Offline Handwritting Datasets}

\subsection{Genetic CNN}


With the rise in number of layers in the CNNs, deeper neural networks are more difficult to train, and using skip connections proved invaluable and allowed 
for easier training of substantially deeper networks. \cite{5}. Those skip connections enable identity functions to be learned easily where needed.
Those skip connections can be manually selected, but since the number of possible skip connections grows exponentially, and because evaluating each model can take a long time, 
in practice its impossible to try each possible architecture.
Many handcrafted CNN architectures axist, but since manually searching the space of all architectures is impractical, it gives a great incentive for automatic search 
for a good architecture.

A possible solution for finding a good architecutre automatically is by using some kind of metaheuristic. 
One paper proposes an encoding method of representing each network structure as a fixed length binary string \cite{4}. 
They define genetic operations: selection, mutation and crossover to generate better suited individuals and eliminate weak ones.
The fitness of an individual is defined through as their recognition accuracy, which is gathered through evaluation on a given reference dataset.
The learnt structures are transferrable to other datasets image-baseed datasets.

Another approach focuses on automatically constructing CNN architectures for an image classification task based based on Cartesian genetic programming (CGP).
They use convolutional blocks and tensor concatenation as the node functions in CGP. Their results are comparable with handcrafted state-of-the-art models \cite{5}.

\section{DoubledMNIST Dataset}

\section{Network representation}
\label{sec:repr}

\textcolor{red}{ovde treba opisati kako je enkodirana mreza kao binarni string}



\section{Method}

\textcolor{red}{da spomenemo izvor ideje?}

The core idea of the genetic algorithm is to get a good solution to a problem by generating increasingly better solutions through the process of evolution.
Network architecture is encoded as a binary string of fixed length. That string represents a gene of an individual.
Individuals from the population have a higher chance to pass on their genes to the next generation if they are more fit for a given task.
Through many generations its expected to arrive to a population that has many good individuals and the best individual out of that last generation represents the solution.

The evolution process consists of selection, mutation and crossover. 
The selection process allows stronger individuals to be perserved, and for weaker individuals to be eliminated.
Crossover process combines genes of two individuals to create individuals for the new generation.
Mutation randomly changes a gene of an individual, thus introducing more variety within a population.


\begin{algorithm}[H]
    \SetAlgoLined
    \textbf{Input:} the testing dataset $T$, number of generations $G$, number of individuals in each generation $N$,
    mutation and crossover probabilities $p_M$ and $p_C$, mutation parameter $q_M$ and crossover parameter $q_C$.

    \textbf{Initialization:} generate randomized individuals, train them and compute their fitness (evaluate classification accuracy)\;
        \For{t = 1,...,$G$}{
            \textbf{Selection:} perform selection using a rulet method to p\;
            \textbf{Crossover:} perform crossover with probability $p_C$ and crossover parameter $q_c$\;
            \textbf{Mutation:} perform mutation on individuals which have not had crossover with probability $p_M$ and mutation parameter $q_M$\;

            \textbf{Construction:} construct CNN from the gene encoding it\;
            \textbf{Inheritance:} inherit the segment weights from the most similar individual of the last generation\;
            \textbf{Training:} train the constructed networks, where number of epochs depends on number of inherited segments\;
            \textbf{Evaluation:} evaluate all individuals to get their fitness\;
     }
     \textbf{Output:} individuals of the final generation and their classification accuracy.
     \caption{Genetic algorithm for generating the appropriate network architecture}
\end{algorithm}

We denote the $n$-th individual in generation $t$ as $M_{t,n}$ and fitness of that individual as $f_{t,i}$.

\subsection{Initialization phase}
The initial generation of individuals is generated by assigning each bit in the binary string a random value from Bernoulli distribution $\mathcal{B}(0.5)$.
Then all individuals from the initial generation are fully trained on the training dataset and evaluated on a testing dataset to get their fitness.
After that, the genetic evolution process is repeated for a set number of generations.

\subsection{Selection phase}
The most common selection methods are roulet selection and tournament selection. 
Here we use roulet selection where the least fit individual is always eliminated.
In roulet selection, each individual has a chance to pass on its genes to the next generation, 
where the probability of that event is proportional to the individuals fitness in comparison to the fitness of all other individuals.
The sampling is performed $N$ times (number of individuals in each generation) with replacement, thus each individual may be selected multiple times.
The probability of individual $M_{t,n}$ passing the selection is equal to $f_{t,i} / \sum_{i=1}^{N} (f_{t,i} - f_{t,min}$), where $f_{t, min} = \min_{i=1}^{N} {f_{t,i}}$.

\subsection{Crossover phase}
The crossover process combines the genes of two individuals to create one or two new individuals.
Here crossover of two individuals always results in two new individuals.
When two individuals are in a crossover they swap a whole segment.
That way the learned connections within a segment are kept through the generations, while still introducing more variaty in individuals.
Candidates for crossover are pairs of individuals $(M_{t,2i-1}, M_{t,2i}), \forall i=1,..,\lfloor N/2\rfloor $.
The probability of crossover between two individuals is $p_C$, and the probability of two segments being swapped is $q_C$.

\subsection{Mutation phase}
Mutation can occur only if an individual didn't go through the crossover process.
In that case the individual starts going through mutation with probability $p_M$.
Then each bit in the individuals string representation has a low chance of being inverted, defined in $q_M$.
Mutation ensures additional variaty in individuals and allows for exploring different architectures within each segment.

\subsection{Construction phase}
The binary encoded  string of each individual is parsed and the graph is constructed, where graph nodes represent the convolution operations.
Connections withing graph nodes signal that there is a connection between those two nodes.
The whole CNN is constructed by parsing the binary string according to the representation discussed in \ref{sec:repr}. 
The end of the network always consists of a flatten layer, followed by a dense (fully connected) layer with 32 units, and finally a dense 
layer with softmax activation and number of units equal to the number of possible classes.

\subsection{Inheritance phase}
The novelty in this approach stems from the observation that there isn't always a need to train each network from scratch.
In the most trivial case, the network in the new generation is identical to the network in the last generation and there is no need to train it again.
In that situation, the weights of the new individual can be inherited (copied) from the individual of the last generation.
Though the probability of the trivial case is small, we can extend the idea to other cases.

In case the new individual has the first $n$ out of $m$ (where $n < m$) segments the same as some individual from the last generation, we can still leverage that similarity
by inheriting the segments from an individual of the last generation that are unchanged in the individual of the new generation.
In this instance, the network already has learnt the lower level features, and only the last $m-n$ segments need to be trained.
The first $n$ segments are not frozen so that they can freely adapt to the new architecture. 

Since the lower segments are already trained, the network needs less time to converge, thus we can reduce the number of epochs in training the network.
That observation leads to an implementation that allows us to double the execution time of the algorithm that performs architecture search.
The results are shown in section \ref{sec:eval}.

\subsection{Training phase}
Training is performed on a constructed CNN model for a set amount of epochs. 
The more segments were inherited in inheritance phase, the less number of epochs is needed to train the model.
Training is done on the training dataset which is the same for all individuals.

\subsection{Evaluation phase}
Evaluation phase is performed to get the fitness of an individual.
The trained model is evaluated on a testing dataset to get its classification accuracy, which is used as its fitness.

%todo moraju od pocetka segmenti opisi

\section{Evaluation}
\label{sec:eval}



\section{Conclusion}


\begin{thebibliography}{99}
\bibitem{1}   
\textbf{Cohen, G., Afshar, S., Tapson, J., van Schaik, A.} EMNIST: an extension of MNIST to handwritten letters. \emph{arXiv preprint arXiv:1702.05373.}, 2017.

\bibitem{2} 
\textbf{Floreano, D., Dürr, P., Mattiussi, C.} Neuroevolution: from architectures to learning. \emph{Evolutionary intelligence}, 1(1), 47-62, 2008.

\bibitem{3} 
\textbf{Voß, S., Martello, S., Osman, I. H., Roucairol, C. (Eds.).} Meta-heuristics: Advances and trends in local search paradigms for optimization. \emph{Springer Science and Business Media.}, 2012.

\bibitem{4}
\textbf{Xie, L., Yuille, A. } Genetic cnn. \emph{In Proceedings of the IEEE International Conference on Computer Vision (pp. 1379-1388).}, 2017.

\bibitem{5}
\textbf{Suganuma, M., Shirakawa, S., Nagao T. } A genetic programming approach to designing convolutional neural network architectures. \emph{In Proceedings of the Genetic and Evolutionary Computation Conference (pp. 497-504).}, 2017.

\bibitem{5}
\textbf{He, K., Zhang, X., Ren S., Sun J.} Deep Residual Learning for Image Recognition. \emph{The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2016.

\end{thebibliography}



\end{document}
